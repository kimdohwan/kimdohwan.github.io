---
layout: post
title:  "코드 리뷰(임시)"
categories: Etc
---

### Wadiz rest api

#### URL

```python
urlpatterns = [
    path('', apis.UserList.as_view()),
    path('signin/', apis.AuthToken.as_view()),
    path('detail/<int:pk>/', apis.UserDetail.as_view()),
    path('signup/', apis.UserList.as_view(), name='signup'),
    path('activate/<str:uidb64>/<str:token>/', apis.UserActivate.as_view(), name='activate'),
    path('change-info/', apis.UserDetail.as_view()),
    path('myinfo/', apis.UserInfo.as_view()),
]
```

#### View

```python
class UserList(APIView):

    def get(self, request): # User List
        serializer = UserSerializer(User.objects.all(), many=True)
        return Response(serializer.data, status=status.HTTP_200_OK)

    def post(self, request): # User Create
        serializer = UserSerializer(data=request.data)
        if serializer.is_valid():
            serializer.save()
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)


class UserDetail(APIView):
    def get(self, request, pk): # User Get
        serializer = UserDetailSerializer(User.objects.get(pk=pk))
        return Response(serializer.data, status=status.HTTP_200_OK)

    permissions = (permissions.IsAuthenticated,)

    def patch(self, request, *args, **kwargs): # User Update
        user = User.objects.get(username=request.user)
        serializer = UserChangeInfoSerializer(user, data=request.data)
        if serializer.is_valid():
            serializer.save()
            return Response(serializer.data, status=status.HTTP_200_OK)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

    def post(self, request): # User Delete
        user = User.objects.get(username=request.user)
        serializer = UserDeleteSerializer(user, data=request.data)
        if serializer.is_valid():
            user.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        return Response(serializer.errors, status=status.HTTP_403_FORBIDDEN)
    
    class UserInfo(generics.RetrieveAPIView): 
        permission_classes = (permissions.IsAuthenticated,)

        def get(self, request): # User Get(authenticated)
            user = User.objects.get(username=request.user)
            serializer = UserSerializer(user)
            return Response(serializer.data, HTTP_200_OK)
```

#### serializers
```python
class UserSerializer(serializers.ModelSerializer):
    username = serializers.EmailField(
        validators=[UniqueValidator(queryset=User.objects.all())])
    password = serializers.CharField(
        max_length=12, min_length=1, allow_blank=False, write_only=True)
    nickname = serializers.CharField(
        max_length=20, validators=[UniqueValidator(queryset=User.objects.all())])
    funding_set = FundingSerializer(many=True, required=False)
    like_products = ProductLikeSerializer(many=True, required=False)

    class Meta:
        model = User

        fields = (
            'pk',
            'username',
            'password',
            'nickname',
            'img_profile',
            'like_products',
            'funding_set'
        )

    def validate_password(self, value):
        if value == self.initial_data.get('check_password'):
            return value
        raise ValidationError('(password, check_password) 불일치')

    def create(self, validated_data):
        user = User.objects.create_user(
            username=validated_data['username'],
            password=validated_data['password'],
            nickname=validated_data['nickname'],
        )
        user.is_active = False
        user.save()

        message = render_to_string('user/account_activate_email.html', {
            'user': user,
            'domain': 'ryanden.kr',
            'uid': urlsafe_base64_encode(force_bytes(user.pk)).decode('utf-8'),
            'token': account_activation_token.make_token(user),
        })

        mail_subject = 'test'
        to_email = user.username
        email = EmailMessage(mail_subject, message, to=[to_email])
        email.send()

        return validated_data


class UserChangeInfoSerializer(serializers.ModelSerializer):
    username = serializers.EmailField(
        validators=[UniqueValidator(queryset=User.objects.all())], required=False)
    password = serializers.CharField(
        required=True, max_length=12, min_length=1, write_only=True)
    nickname = serializers.CharField(
        max_length=20, validators=[UniqueValidator(queryset=User.objects.all())], required=False)

    class Meta:
        model = User
        fields = (
            'pk',
            'username',
            'password',
            'nickname',
            'img_profile',
        )

    def validate_password(self, value):
        if self.instance.check_password(self.initial_data.get('password')):
            return value
        raise ValidationError('password 가 틀렸습니다')

    def validate(self, data):
        if self.initial_data.get('new_password'):
            if self.initial_data.get('new_password') == self.initial_data.get('check_password'):
                data['password'] = make_password(self.initial_data.get('new_password'))
            else:
                raise ValidationError('(new_password, check_password) 불일치')
        else:
            data['password'] = make_password(self.initial_data.get('password'))
        return data


class UserDetailSerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = (
            'username',
            'nickname',
            'img_profile',
        )


class UserDeleteSerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = (
            'password',
        )

    def validate_password(self, value):
        if self.instance.check_password(value):
            value
            return value
        raise ValidationError('password 가 틀렸습니다')
```


### Brunch RSS Feed

#### views.py

```python
def create_feed_url(request):
    input_word = request.GET.get('input_word')  # 검색한 단어
    input_option = request.GET.get("input_option", "keyword")  # 검색 옵션(기본 설정: keyword)

    keyword = input_word if input_option == 'keyword' else None
    writer = input_word if input_option == 'writer' else None

    # 검색결과가 존재여부 검사를 위해 크롤러 객체 생성
    crawler = Crawler(keyword=keyword, writer=writer)

    # search_result 가 True 일 경우 검색 결과 존재
    if crawler.search_result:

        # 백그라운드에서 크롤링을 실행하게 됨
        task_for_crawling.delay(keyword=keyword, writer=writer)

        root_url = request.META['HTTP_REFERER']
        feed_uri = f'feeds/{input_option}/{input_word}/'

        # django messages return Feed URL
        messages.add_message(
            request,
            messages.INFO,
            'Feed URL',
        )
        messages.add_message(
            request,
            messages.SUCCESS,
            root_url + feed_uri,
        )

    # no result from user's 'input word'
    else:
        messages.add_message(
            request,
            messages.WARNING,
            f'{"키워드" if keyword else "작가"} {input_word} 에 대한 검색결과 없음'
        )

    return redirect('articles:index')
```

#### crawler.py

```python
class Crawler:
    def __init__(self, keyword=None, writer=None):
        self.keyword = keyword
        self.user_id = writer

        self.driver = None  # 크롤링에 사용할 selenium 드라이버
        self.html = None  # 검색결과 목록 페이지의 html
        self.article_txid_list = None  # 각각의 글들에 대한 id 를 담은 list
        self.cleand_txid_list = None  # DB에 존재하는 글을 '제외한' 글의 txid list

        self.obj_keyword = None  # keyword 검색 시, manytomany 에 추가 시켜줄 keyword 객체

    def crawl(self):

        html_s = time.time()
        self.get_search_result()  # self.html 셋팅
        html_e = time.time()
        print('검색결과 페이지 크롤링 시간', html_e - html_s)

        # 검색어가 정확히 입력된다면 article, writer, keyword 저장 후 True
        if self.html:  # 검색결과가 존재한다면
            self.get_article_txid()  # 글의 url 링크를 생성할 수 있는 txid(article id 에 해당)를 셋팅

            # request 수 제한을 위해 최신글 5개만 크롤링하도록 임의로 설정
            self.article_txid_list = self.article_txid_list[:10]

            self.remove_existed_article()  # 중복검사를 통해 이미 저장한 아티클은 제외한다.

            if self.keyword:  # Keyword 검색일 경우만 해당되는 처리(ManyToMany)
                self.set_keyword()

            self.crawl_detail_and_save()  # 상세페이지의 내용을 크롤링한 후 저장.

    @property
    def search_result(self):
        """
        크롤링 실행하기에 앞서, 검색결과의 존재 여부를 T/F 로 리턴
        """

        self.get_search_result()

        # html 이 셋팅되었다면 검색결과가 존재하며, None 이라면 검색결과 없음
        if self.html:
            return True
        return False

    def get_search_result(self):
        """
        javascript 로딩을 위해 selenium 드라이버 사용,
        검색결과가 존재 한다면 self.html 에 결과 페이지의 html source 할당
        """
        self.driver = set_headless_driver()  # 셀레니움 드라이버 셋팅

        try:
            if self.keyword:  # 키워드로 검색할 때는 최신순으로 정렬된 페이지를 크롤링
                url = f'https://brunch.co.kr/search?q={self.keyword}'
                self.driver.get(url)
                time.sleep(2)
                self.driver.find_elements_by_css_selector('span.search_option > a')[1].click()  # 최신순 정렬 클릭
                time.sleep(1)  # 최신순으로 누르고 기다리는 시간

            elif self.user_id:  # 작가의 글 페이지를 크롤링
                url = f'https://brunch.co.kr/@{self.user_id}#articles'
                self.driver.get(url)
                time.sleep(2)
                self.driver.find_element_by_css_selector('div.wrap_article_list')
            html = self.driver.page_source

        except (NoSuchElementException, ElementNotVisibleException):
            html = ''  # find_element 실패 시 Exception -> empty 'html'

        self.driver.close()
        self.html = html

    # 글 상세 페이지로 넘어갈 때 사용할 article_txid(아이디+글번호)를 리스트로 담아 리턴
    def get_article_txid(self):
        soup = BeautifulSoup(self.html, 'lxml')
        li_article = soup.select('div.wrap_article_list > ul > li')

        article_txid_list = []
        for li in li_article:
            article_txid = li['data-articleuid']
            article_txid_list.append(article_txid)

        self.article_txid_list = article_txid_list

    def remove_existed_article(self):
        """
        이미 존재하는 Article 인지 검사 후, 존재하는 Article 은 크롤링 목록에서 제거한다.
        제거한 목록을 self.clead_txid_list 값으로 셋팅한다.
        """

        cleand_txid_list = [txid for txid in self.article_txid_list]

        for article_txid in self.article_txid_list:
            try:
                obj = Article.objects.get(article_txid=article_txid)
            except Article.DoesNotExist:
                obj = None

            # 이미 존재하는 아티클이라면 크롤링 할 리스트에서 제외
            if obj:
                cleand_txid_list.remove(article_txid)

        self.cleand_txid_list = cleand_txid_list

    def set_keyword(self):
        """
        Keyword 검색에 해당하는 경우 실행되는 메서드.
        DB에 존재하는 Article 의 경우, 검색된 Keyword 를 추가.

        사례:
            상황: 'A'라는 글이 'python' 이라는 키워드로 크롤링 되어 이미 DB에 존재요청.
            -> 'django' 키워드로 검색 시 'A' 가 또 검색됨.
            -> 글 'A' 는 이미 DB에 존재하므로 'django'라는 Keyword 만 새로 추가해준다.
            결과: 글 'A' 는 'python', 'django' 라는 Keyword 를 Foreign Key 로 갖게 됨.
        """
        self.obj_keyword, _ = Keyword.objects.get_or_create(keyword=self.keyword)

        existed_article_txid = set(self.article_txid_list) - set(self.cleand_txid_list)
        existed_article = [Article.objects.get(article_txid=article_txid) for article_txid in existed_article_txid]

        for article in existed_article:
            # 이미 존재하는 아티클에 키워드가 추가되어있지 않다면 키워드를 추가한다.
            if not article.keyword.filter(keyword=self.keyword).exists():
                article.keyword.add(self.obj_keyword)
                article.save()

    def crawl_detail_and_save(self):
        """
        asyncio 모듈을 활용하여 비동기 를 통해 각각의 글(Article)들을 저장

        process:
            1. loop.run_until_complete : async loop 실행
            2. create_task_async() : 수행할 TASK 객체(detail_crawl() 수행) 생성
            3. detail_crawl() : Article 의 상세페이지 크롤링 및 저장
        """

        async def detail_crawl(url, article_txid):
            print(f'Send request .. {url}')

            async with aiohttp.ClientSession() as sess:
                async with sess.get(url) as res:
                    r = await res.text()  # 다음 TASK 를 실행하는 지점 / 응답이 온 순서대로 작업 실행하는 지점.
            soup = BeautifulSoup(r, 'lxml')

            print(f'Get response .. {url}')

            title = soup.find('meta', {'property': 'og:title'}).get('content')
            content = soup.select_one('div.wrap_body').prettify()
            media_name = soup.find('meta', {'name': 'article:media_name'})['content']

            try:  # 구독자 수 크롤링
                num_subscription = int(''.join(re.findall(
                    r'\w',
                    soup.select_one('span.num_subscription').text
                )))

            # 브런치에서 구독자 수가 1만을 초과하는 경우 '4.3만' 과 같이 표기된다.
            # 이때 int 형변환 시 '.' 과 '만' 때문에 ValueError 발생하므로 적절한 예외 처리 필요
            #   - '만' 을 '0000'으로 replace
            #   - 소수점 밑자리 숫자는 생략 처리
            except ValueError:
                char_num_subscription = soup.select_one('span.num_subscription').text
                trunc_part = ''.join(re.findall(r'.\d+', char_num_subscription))
                num_subscription = int(char_num_subscription.replace('만', '0000').replace(trunc_part, ''))

            published_time = re.findall(
                r'(\S+)\+',
                soup.find('meta', {'property': 'article:published_time'})['content'],
            )[0]
            user_id, text_id = re.findall(
                r'/@(\S+)/(\d+)',
                soup.find('meta', {'property': 'og:url'})['content']
            )[0]

            writer, _ = Writer.objects.update_or_create(
                user_id=user_id,
                defaults={
                    'media_name': media_name,
                    'num_subscription': num_subscription
                }
            )

            # article 생성
            article_without_keyword = Article.objects.create(
                title=title,
                content=content,
                article_txid=article_txid,
                published_time=published_time,
                text_id=text_id,
                writer=writer
            )

            # article 에 keyword 추가(ManyToMany) - keyword 검색일 경우에 한하여
            if self.keyword:
                article_without_keyword.keyword.add(self.obj_keyword)

            print(f'저장 완료 {url}')

        # futures 에 Task 할당(url, 중복검사 완료된 checked_article_txid)
        async def create_task_async():
            """
            TASK 객체를 생성한 후, gather() 을 통해 TASK(FUTURE) 전달 및 코루틴 실행
            """
            brunch_url = 'https://brunch.co.kr/'

            # article_txid 문자열 변환을 통해 url 링크 생성(detail_crawl 의 인자가 됨)
            futures = [
                asyncio.ensure_future(
                    detail_crawl(
                        url=brunch_url + '@@' + article_txid.replace('_', '/'),
                        article_txid=article_txid
                    )
                ) for article_txid in self.cleand_txid_list
            ]

            await asyncio.gather(*futures)

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(create_task_async())

```

#### Feed 생성

```python
class MyDefaultFeed(DefaultFeed):
    content_type = 'application/xml; charset=utf-8'


# 장고 피드를 상속받아 필요한 부분을 커스터마이징
class MyFeed(Feed):
    feed_type = MyDefaultFeed

    def item_title(self, item):
        return item.title  # 포스팅의 제목 설정(피드 이름 아님)

    def item_description(self, item):
        return item.content  # 글 내용

    def item_pubdate(self, item):
        return item.published_time  # 글쓴 시간

    def item_link(self, item):  # 내 피드 페이지가 아닌 브런치 페이지로 이동
        return f'https://brunch.co.kr/@@{item.article_txid.replace("_", "/")}'

    def get_feed(self, obj, request):
        self.title = self.brunch_title + obj  # 피드의 이름 설정(포스팅 제목 아님)
        feed = super().get_feed(obj, request)

        # celery 에서 크롤링/피드생성 작업을 하는 동안 처리해줄 피드 메시지
        if not feed.items:
            feed.feed['title'] = f'{obj} 에 대한 피드를 생성 중입니다'

        return feed
```

```python
# 커스터마이징한 MyFeed 상속
class KeywordFeed(MyFeed):
    brunch_title = "Brunch 키워드: "  # 피드의 이름에 들어갈 문자열
    link = "/feeds/keyword/"
    description = "등록한 검색어의 최신글들을 업데이트합니다"

    def items(self, keyword):  # Keyword 에 연결된(ManyToMany) Article

        try:
            query_set = Keyword.objects.get(keyword=keyword).articles.all().order_by('-published_time')

        # 크롤링 작업을 모두 마치지 못한 상태에서 피드를 호출할 경우
        # 빈 리스트를 리턴 한 후, 피드 생성 중이라는 메시지를 Feed 의 title 에 셋팅(Myfeed)
        except Keyword.DoesNotExist:
            query_set = []

        return query_set
        # return Keyword.objects.get(keyword=keyword).articles.all().order_by('-published_time')

    def get_object(self, request, *args, **kwargs):
        return str(kwargs['keyword'])  # 검색된 키워드 전달
```

### Place for eldery

#### python manage.py setapidata

```python
# ./manage.py setapidata 실행
class Command(BaseCommand):
    def handle(self, *args, **options):

        try:
            # transaction 을 사용해 Exception 발생 시, 이전 DB 상태에 영향 주지 않도록 처리
            with transaction.atomic():
                # facility 항목 모두 지운 후 진행
                Facility.objects.all().delete()

                # get_api_data() 로부터 시설 리스트를 받아옴
                api_data = get_api_data()

                # Facility obj 를 생성해 list 에 할당
                # 만약 api_data 가 None 일 경우, 이 구문에서 TypeError 발생
                objs = [Facility(**kwargs) for kwargs in api_data]

                # bulk_create() 를 사용해 한번에 저장하도록 처리
                Facility.objects.bulk_create(objs)

                print('{} 개의 Facility 저장.'.format(Facility.objects.all().count()))

        # TypeError: get_api_data() 에서 return 값이 없는 경우(open_api requests 실패)
        except TypeError:
            print("Can't set database")
```

#### get_api_data()

```python
# - 기본 파라미터
#     'KEY': 발급받은 api key,
#     'Type': json 또는 xml
#     'pindex': 페이지 인덱스
#     'pSize': 최대 1000개까지 요청 가능
# - 요청 가능 파라미터
#     'SIGUN_CD': 시/군 코드
#     'SIGUN_NM': 시/군 한글명
# - 응답 코드 종류
#     구분	코드	설명
#     ERROR	300	필수 값이 누락되어 있습니다. 요청인자를 참고 하십시오.
#     ERROR	290	인증키가 유효하지 않습니다. 인증키가 없는 경우, 홈페이지에서 인증키를 신청하십시오.
#     ERROR	336	데이터요청은 한번에 최대 1,000건을 넘을 수 없습니다.
#     ERROR	333	요청위치 값의 타입이 유효하지 않습니다.요청위치 값은 정수를 입력하세요.
#     ERROR	310	해당하는 서비스를 찾을 수 없습니다. 요청인자 중 SERVICE를 확인하십시오.
#     ERROR	337	일별 트래픽 제한을 넘은 호출입니다. 오늘은 더이상 호출할 수 없습니다.
#     ERROR	500	서버 오류입니다. 지속적으로 발생시 홈페이지로 문의(Q&A) 바랍니다.
#     ERROR	600	데이터베이스 연결 오류입니다. 지속적으로 발생시 홈페이지로 문의(Q&A) 바랍니다.
#     ERROR	601	SQL 문장 오류 입니다. 지속적으로 발생시 홈페이지로 문의(Q&A) 바랍니다.
#     INFO	000	정상 처리되었습니다.
#     INFO	300	관리자에 의해 인증키 사용이 제한되었습니다.
#     INFO	200	해당하는 데이터가 없습니다.

BASE_URL = 'https://openapi.gg.go.kr'
API_NAME = 'OldPersonRecuperationFacility'
API_KEY = settings.secrets_base['API_KEY']
SUCCESS_CODE = ['INFO-000', 'INFO-200']


def get_api_data():
    url = BASE_URL + '/' + API_NAME
    api_data = []

    # pindex 1 부터 데이터가 없을 때까지 while loop 진행
    pindex = 1
    while True:
        payload = {
            'KEY': API_KEY,
            'Type': 'json',
            'pindex': pindex
        }

        # ConnectionError 발생 시 requests session 설정을 바꿔 진행
        try:
            res = requests.get(url, params=payload)
        except requests.ConnectionError:
            session = requests.Session()
            retry = Retry(connect=3, backoff_factor=0.5)
            adapter = HTTPAdapter(max_retries=retry)
            session.mount('https://', adapter)
            res = session.get(url, params=payload)

        j = json.loads(res.text)

        # 더 이상 불러올 데이터가 없거나(INFO-200), 잘못된 요청일 경우 while loop 중단
        if API_NAME not in j.keys():
            code = j['RESULT']['CODE']
            message = j['RESULT']['MESSAGE']

            # 데이터를 모두 불러온 경우 return
            if code in SUCCESS_CODE:
                break

            # 중단된 이유를 나타내는 code 와 message 출력 후 중단
            print(code, message)
            # None 을 리턴할 경우, manage.py setapidata 실행 시 TypeError 발생함
            return None

        # 시설 리스트를 json(j) 에서 얻은 후, 날짜 포멧을 변경하는 함수 실행
        facility_list = j[API_NAME][1]['row']
        facility_list = change_date_format(facility_list)

        # return 될 api_data 에 시설 리스트를 추가
        api_data += facility_list

        pindex += 1

    return api_data


# 날짜를 저장 가능한 포멧으로 변경
# 20111230 과 같이 들어오는 포멧을 2011-12-30으로 변환하는 함수
def change_date_format(facility_list):
    # l_data: 인허가일자
    # c_date: 폐업일자
    for facility in facility_list:
        l_date = facility['LICENSG_DE']
        if l_date is not None:
            facility['LICENSG_DE'] = l_date[:4] + '-' + l_date[4:6] + '-' + l_date[6:]
        c_date = facility['CLSBIZ_DE']
        if c_date is not None:
            facility['CLSBIZ_DE'] = c_date[:4] + '-' + c_date[4:6] + '-' + c_date[6:]
    return facility_list
```

